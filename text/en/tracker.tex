\chapter{Tracker}

We consider a tracker to be an algorithm for detection a position of an object in
an image. We present a few tested trackers and their results in this task.
Firstly, we provide a short description of a simple straightforward tracker and
then we describe more complicated trackers.

Tracker returns an object position in the image. We differentiate between two
types of tracking algorithms: \emph{detection based algorithms} and
\emph{sequence-based algorithms}. Detection based algorithms detect an object
on each image separately. On the other hand, sequence-based algorithms obtain,
store and process information from a sequence of the past images and use them
for more accurate results, while requiring same or even less computation time
compared to the detection based algorithms. For this task, we test few trackers
in both categories. Evaluation of the trackers is at the end of this chapter.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section {Detection based algorithms}

We denote all trackers, which detect an object on each image separately, as
detection based algorithms. These algorithms use only information obtained from
the current image and the information from the image used while the tracker was
initialized.

\subsection{Simple Background Tracker}

\emph{Simple Background Tracker} takes a photo of the background at the
beginning and we denote it as a \emph{pattern}. In order to detect an object in
an \emph{image}, a comparison of the \emph{image} and the \emph{pattern} is done.
We make this comparison by taking a sum of an absolute difference for each
color channel (Red, Green, Blue) in the images for each pixel.

As a result, we get a mask, where higher values mean the bigger difference between
the colors of the \emph{pattern} and the \emph{image} at given pixel. We assume it
is caused by an object in front of the camera.

One may expect to get the mask by \emph{tresholding} (see the results in
\ref{fig:simple-background-first-threshold}). There is still a lot of noise.
The noise is in the form of small dots and lines. Therefore we use
\emph{blurring} to remove the noise. Then we \emph{threshold} it again to get
binary mask (see the results in \ref{fig:simple-background-second-threshold}).
At this point, we will find a contour with the biggest area using OpenCV
library. The center point of the rectangle of this contour will be our
estimation of the position of our object in the image. The whole process is in
the figure \ref{fig:simple-background-tracker}.

In the image, it is showed that between the photo of the background and the photo
of the object the light slightly changed (at the edge of the puzzle pieces). It
is causing noise, which we reduce by thresholding and blurring the image.

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=\linewidth]{img/simple_background/background.jpg}
    \caption{Background}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=\linewidth]{img/simple_background/object.jpg}
    \caption{Object}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=\linewidth]{img/simple_background/rgb-diff.jpg}
    \caption{Sum of diffs in each channel (RGB)}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=\linewidth]{img/simple_background/first-thresh.jpg}
    \caption{Thresholded}
    \label{fig:simple-background-first-threshold}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=\linewidth]{img/simple_background/blurring.jpg}
    \caption{Blurred}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=\linewidth]{img/simple_background/second-thresh.jpg}
    \caption{Thresholded}
    \label{fig:simple-background-second-threshold}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=\linewidth]{img/simple_background/result.jpg}
    \caption{Bounding box of the largest contour found.}
  \end{subfigure}
  \caption{Process of the simple background tracker}
  \label{fig:simple-background-tracker}
\end{figure}

As an advantage of this algorithm, we consider its simplicity and
straight-forward implementation. Furthermore, with a static background with
only one object moving it can reliably track an object without any information
about it.

On the contrary, it can not recover even from a little light changes or camera
movement. Also, an object moved by a human, for example, cannot be tracked
reliably, since the tracker recognizes the hand as a moving object
too.

\subsection{HSV tracker}

HSV tracker uses an idea of tracking an object by its color. Given an input
object described by a bounding box, we find the average color within the
bounding box. On a position request, we return a center of the largest area
with the color of the object.

We choose the color coding via HSV (Hue, Saturation, Value). Unlike the RGB (Red,
Green, Blue) coding it can describe a color as hue value, not triple values of
mixed colors. The approach of HSV color coding preserves one value -- hue
value,  even though the color is lighter or darker (like shadows in the image).
On the other hand in the RGB coding shadows may cause a difference in all three
parts of coded color. Therefore an object description can be simplified
to one value.

\begin{figure}[h!]\centering
\includegraphics[width=0.65\textwidth]{img/hsv-cylinder.png}
\caption{"HSV cylinder" by SharkD is licensed under CC BY 3.0}
\end{figure}

We describe the algorithm in a few steps. Firstly, we convert the template
image (bounding box) from the RGB color space to HSV. Then we choose an average
color in the template. Since the coding of hue part is placed in the circle, it
is not enough to take a commonly used average. It would cause that image full
of warm red (the hue value of this color is circa equal to 15) and cool red
(the hue is circa 345) would average to mid cyan (hue: 180), instead of Red
(hue: 0).

To get a more reasonable average, we take the hue value of each pixel as an
unit vector (the hue value is encoded as the angle between given vector
and vector $(1, 0)$). We sum these vectors and get a vector $(x, y)$. We find a
corresponding angle for this vector using $arctang2$ function. The following
formulas describe the process of getting an average angle.

\todo[inline]{Obrazok na ten vzorceky a tak}

$$
\begin{aligned}
x &= \sum_\alpha cos \alpha \\
y &= \sum_\alpha sin \alpha \\
\alpha_{avg} &= arctang2(y, x)
\end{aligned}
$$

The use of this algorithm for one-colore object is displayed in the figure
\ref{fig:hsv-tracker}. We consider as a disadvantage that no other objects of
the same color can be placed in the view of the camera.

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=\linewidth]{img/hsv/initial.jpg}
    \caption{Initial image with the selected object}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=\linewidth]{img/hsv/object.jpg}
    \caption{Image with the moved object}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=\linewidth]{img/hsv/mask.jpg}
    \caption{Mask created by looking for similar colors}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=\linewidth]{img/hsv/result.jpg}
    \caption{Bounding box of the largest contour found.}
  \end{subfigure}
  \caption{Process of the HSV tracker}
  \label{fig:hsv-tracker}
\end{figure}

\subsection {Pattern matching}

Pattern (or template) matching algorithm slides over the input image and
compares the template with the patch of the input image. By \emph{patch} we
mean an image cropped to the size of the template (displayed in the figure
\ref{fig:patternmatching-naming}).

For a comparison between an image and the patch we look for a function, which
tell us how different to each other they are. Such a function is usually called
a loss fuction. We use a function $R$ based on square distance as our loss
function. More precisely, we take the sum of square distances of all pairs of
corresponding pixels in the template and the patch. Therefore, we compute $R$
as:

$$
R(x, y) =
%\frac{
\sum_{x', y'} ||T(x', y') - I(x + x', y + y')||^2
%}{
%\sqrt{\sum_{x', y'} T(x', y')^2 \cdot \sum_{x', y'} I(x+x', y+y')^2}
%}
$$
where $x'$ and $y'$ denote points from the neighbourhood of the $x, y$. $T$
denotes our pattern and $I$ denotes the image. 

\begin{figure}[h]
	\centering
	\def\svgwidth{0.9\linewidth}
	\input{img/pattern_matching/naming.pdf_tex}
	\caption{Naming convention showed on an example}
	\label{fig:patternmatching-naming}
\end{figure}

From the computed value for each pixel the one with the lowest value
(i.e. shortest distance) is our estimation of the position of the object.

Finally, we choose an appropriate representation for the pixels (i.e. what
precisely $T(x, y)$ and $I(x, y)$ stand for). In general, we can choose any
reasonable vector (such as a tuple of RGB channels). We choose to use a standard
conversion to grayscale ($T(x, y)$ and $I(x, y)$ thus give an intensity after
such conversion).

Because this algorithm works with the grayscale images, much information is
lost during conversion. This disadvantage summed up with no ability to
recognize rotated or slightly changed objects results in unsatisfying tracking
results. A process of correct match and also incorrect one is displayed in the
figure \ref{fig:pattern-matching-tracker}. We can see the masks, which displays
the value of the loss function. Lower values of the loss functions are
displayed as darker points. Therefore, the black spots means patches which are
similar to the pattern.

\begin{figure}
  \centering
  \begin{subfigure}{0.48\linewidth}
    \includegraphics[width=\linewidth]{img/pattern_matching/initial.jpg}
    \caption{Initial image with selected object}
  \end{subfigure}
  \begin{subfigure}{0.48\linewidth}
    \includegraphics[width=\linewidth]{img/pattern_matching/blackwhite.jpg}
    \caption{Converting it to black white}
  \end{subfigure}
  \begin{subfigure}{0.48\linewidth}
    \includegraphics[width=\linewidth]{img/pattern_matching/mask.jpg}
    \caption{Mask created by applying metric}
  \end{subfigure}
  \begin{subfigure}{0.48\linewidth}
    \includegraphics[width=\linewidth]{img/pattern_matching/result-correct.jpg}
    \caption{Darkest point (lowest value) from mask is choosed}
  \end{subfigure}
  \begin{subfigure}{0.48\linewidth}
    \includegraphics[width=\linewidth]{img/pattern_matching/mask-incorrect.jpg}
    \caption{Mask with darkest point on the left}
  \end{subfigure}
  \begin{subfigure}{0.48\linewidth}
    \includegraphics[width=\linewidth]{img/pattern_matching/result-incorrect.jpg}
    \caption{Incorrectly matched pattern}
  \end{subfigure}
  \caption{Process of the pattern matching}
  \label{fig:pattern-matching-tracker}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Sequence-based algorithms}

We denote as sequence-based algorithms a class of tracking algorithms, which
are using information from a sequence of images.

Using the advantage of information from previous frames one could create not only
more stable but also faster trackers. Furthermore, trackers of this class usually preserve identity, which means
that also in the case of multiple moving objects in a frame it remains tracking the original
one.

Examples of the information we can obtain from sequence of the images are:
\begin{itemize}
\item velocity -- from previous images we can estimate the speed and direction of
  the movement. This information can reduce the searching area to smaller one
  and increase the speed of the algorithm.
\item appearance -- the object may rotate and change its shape or color. Tracker able to
  learn can be persistable against such changes
\end{itemize}

An algorithm using this information can cope with occlusion (one object covers
another) -- what detection algorithms are usually not able to do.

In the next sections, we will present a few trackers implemented in the OpenCV. We
provide a short overview of the trackers available.

\subsection*{BOOSTING tracker}
\todo[inline]{Citace?}
Boosting tracker is based on online AdaBoost. It considers a bounding box as
a positive sample and patches of background as negative ones. For a new image, the
classifier runs on every pixel in the neighborhood of the previous location,
scoring every pixel. The location with the highest score is chosen as a new
location. The implementation in OpenCV is based on \citet*{boosting}.

\subsection*{MIL tracker}
The MIL is an abbreviation for Multiple Instance Learning. In comparison to the
BOOSTING tracker, it does not keep only one image of the positive example, but the
set of images. The tracker considers a small neighborhood of the
current position as possible positive examples. It helps the tracker to cope with
the occlusion. OpenCV implementation is based on \citet*{mil}.

\subsection*{KCF tracker}
KCF stands for Kernelized Correlation Filters. Similarly, as the MIL tracker,
it uses more positive samples and their large overlapping regions.
Implementation provided by OpenCV is based on \citet*{kcf}.

\subsection*{TLD tracker}
Tracking, learning and detection, these are the three components of this
tracker. The tracker works frame to frame, and detection is run to correct the
tracker if necessary. The learning estimates detector's errors and updates it
to avoid these errors in the future. The implementation in OpenCV is based on
\citet*{tld}.

\subsection*{MEDIANFLOW tracker}
This tracker focuses on forward-backward error trying to minimize it. The
implementation in OpenCV is based on \citet*{medianflow}.

\subsection*{MOSSE tracker}
MOSSE tracker is proposed for fast object tracking using correlation filter
methods. Firstly it does Fast Fourier Transform for template and the image.
Then an convolution operation is performed between the images and the result is
inverted by Inverse Fast Fourier Transform (IFFT). The position is estimated by
the highest value of the IFFT response. More about the tracker is available in
the paper by \citet*{mosse}.

\subsection*{OpenCV note}
OpenCV-contribute implements all above-described sequence-based trackers. An
overview of OpenCV trackers is provided by \citet*{opencv-trackers}.

\subsection*{Correlation tracker}
We decided to include a tracker implemented in Dlib. This
correlation tracker is based on the paper by \citet*{correlation}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Trackers evaluation}

In previous section we mention many different trackers. They differs in the
approach to the video stream and obtaining informationn from it. As we
mentioned earlier, some of them work with just one image, other ones with
sequence of the images. In this section we will provide a comparison between
them. Our goal is to find the best performing one among them.

Firstly, we list qualities we measure and are important to us:
\begin{itemize}
	\item Accuracy
	\item Speed
	\item Ability to recover from the occlusion
	\item Bonus: ability to track more objects
\end{itemize}

It is important to have an \emph{accurate} tracker. The accuracy of the tracker highly
impact on the results of the localization and small imprecision may results in
estimating a position of the object few centimenters away.

Complexity of the tracker may impact its \emph{speed} negatively. Usually, more
complex tracker take more time to process the image. Our system focus on live
localization process, therefore it is important to have fast tracker. Slow
tracker may cause in love system missing the images, since it is not able to
run "live". Therefore, it may result also in bad accuracy, because the tracker
may loose the object, since it is skipping the images in between.

As our last criteria we set \emph{ability to recover from the occlusion}. As
occlusion we mean partially or fully covering the tracked object by another
object. This ability is not crucial, but it might come handy, if the tracked
object leave the area of the view of the camera.

In order to test our trackers we tested them on videos with same conditions.
The description of the experiments and the results obtained follow.

\subsection{Autonomous robot following a black line}

It is important to know to what purpose will be our project serves. We focus on
the robots and tracing their movement in the 3D space. We decided to test our
tracker in such simplified environment with moving autonomous robot.

Our experiment consisted of the robot following the black line -- in shape of
the oval (figure \ref{fig:robot-oval}). We initialized all trackers to the same
bounding box (which is displayed by blue line on the image), setting our object
to track. Only exception is SIMPLEBACKGROUND tracker, which needs an image
without an object for initialization, therefore we provided it to it
separately.

\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{img/robot-oval.png}
\label{fig:robot-oval}
\caption{Selecting object to track on the video}
\end{figure}

We consider this experiment as quite difficult for the trackers, since the
robot is not only moving, but also quite fast changing its appearance. In a
second or two, its top platform is mirrored.

On the other hand, we provide clear background with no noise, to be able to see
a behaviour of the trackers.

The results of the experiment are available in the table \ref{table:experiment-robot}.

\begin{table}
\centering
\begin{tabular}{l|l|l}
Tracker	& FPS & Accuracy \\
\hline
\input{experiments/trackers.txt}
\end{tabular}
\label{table:experiment-robot}
\caption{Results of the trackers evaluation}
\end{table}

\subsubsection*{Explanation of the measured results}

\emph{FPS} is a shortcut for "Frames Per Second". In the experiment, we measured
how many ticks of the processor the tracker need to process one image in
average. Then we dividided the number of ticks per second by this value. It
means, faster trackers have higher value of FPS, able to process more images in
second. In the table is the mean value of the ticks needed for each image.

This value is only approximate and usually in the whole program with
localization it will be lower, since some of the computation power has to be
used for other parts of the program. We consider a tracker unusable, if it is
not able to process at least 30 images per second, since it is usually the
capturing speed of the cameras.

Tracker speed higly influence the speed of the program itself. Spending all of
the computation power to process an image by tracker results to lagging in the
other parts.

\emph{Accuracy} 
This is more complicated one. To measure accuracy, the best
option would be to tag an object on the each frame by the man. This approach is
highly time consuming and performing experiment on another video would cost an
hours tagging the video.

Our approach is to choose the best performing tracker, which detect the object
correct. In this situation, it does not matter, if it slow. Then for the each
frame of the video we compute the distance between the estimated position of
the representative tracker and the current one. The result is the mean of the
distances between these results of the trackers expressed in pixels.

More specificaly, in our case we choose Simple Background tracker, since it
performed very well (with correct parameters) on the video.

From the results, we can see that MIL, BOOSTING and TLD trackers are too slow
for our application which runs live. HSV tracker was not able to perform at
all, since it is multicolored object and it did not find the object at any
image.

We can also see that, even the MIL and BOOSTING trackers were slow, they
performed quite well. The trackers which have mean value of accuracy more than
100 lost the object at some point and were not aware of it.

As a result from this experiment, we consider CORRELATION, SIMPLEBACKGROUND,
MEDIANFLOW, PATTERMATCHING as usable for our purposes.

To test HSV tracker, we modified the experiment by with placing orange
paper to track on the top of the robot (see figure \ref{fig:hsv-bbox}). The obtained
values for this tracker are listed in the table \ref{table:hsv}. For improving the
performance of the HSV tracker, it is better to select smaller area with the
same color. Therefore, the tracker will track only this are, not the whole
object. The difference between the are of the SIMPLEBACKGROUND and the HSV
tracker is displayed in the figure \ref{fig:hsv-diff}. The red line symbolises
Simple Background tracker and the blue one the HSV tracker. The resulted
accuracy is caused by tracking specific part of the object. The tracker
performed very well.

\begin{table}
\centering
\begin{tabular}{l|l|l}
Tracker	& FPS & Accuracy \\
\hline
\input{experiments/hsv.txt}
\end{tabular}
\label{table:hsv}
\end{table}

\begin{figure}
\centering
\begin{subfigure}{0.48\linewidth}
\includegraphics[width=\linewidth]{img/experiments/hsv-select.png}
\caption{Selecting bounding box for HSV tracker}
\label{fig:hsv-bbox}
\end{subfigure}
\begin{subfigure}{0.48\linewidth}
\includegraphics[width=\linewidth]{img/experiments/hsv-tracking.png}
\caption{Simple background tracker (red) and HSV tracker (blue)}
\label{fig:hsv-diff}
\end{subfigure}
\caption{Testing HSV tracker}
\end{figure}

\todo[inline]{u fps by sa zislo napisat aky stroj}

\subsection{Object under occlusion}

As we have mentioned earlier, it might come handy, to have a tracker which is
able to recover from occlusion. We prepared an experiment, which lost the robot
from the view. We were interested if the tracker is able to report lost object
(or return wrong results) and if it was able to track the object again after
coming back to camera view. The example of the successful recover from the occlusion is displayed in the figure \ref{fig:occlusion}.

\begin{figure}
\centering
\begin{subfigure}{0.48\linewidth}
\includegraphics[width=\linewidth]{img/experiments/occlusion1.png}
\end{subfigure}
\begin{subfigure}{0.48\linewidth}
\includegraphics[width=\linewidth]{img/experiments/occlusion2.png}
\end{subfigure}
\label{fig:occlusion}
\caption{Example of successfully recovering from the full occlusion}
\end{figure}


\begin{table}
\centering
\begin{tabular}{l|l|l}
Tracker & Report of the object lost & Recovery from full occlusion \\
\hline
SIMPLEBACKGROUND & Yes & Yes \\
MIL & No & No \\
BOOSTING & No & No \\
TLD & Sometimes & Yes \\
MEDIANFLOW & Yes & No \\
HSV & Yes & Yes \\
PATTERNMATCHING & No & Yes \\
CORRELATION & No & No \\
MOSSE & Sometimes & No \\
\end{tabular}
\end{table}

\subsection{Tracking multiple objects}

The last test for the trackers is ability to track multiple objects. Simple
detection-based trackers usually choose the best position with highest area
satisfing given condition, therefore tracking multiple object is not possible.

In such situation when tracking multiple object is needed, sequence-based
trackers may outperform detection-based trackers.. We decided to test their
ability to keep tracking the same object, the results are listed in the table
\ref{table:multiple-objects}.

Most of the trackers are not able to recover from mutual occlusion. Denote as
the first object the object behind and the second the one in front of the
first. Then the second object approach first, the tracker of the first object
learn fase of the changing shape of the object and therefore the tracker start
to track the second object. This happened to most of the trackers.

\begin{table}
\centering
\begin{tabular}{l|l|l}
Tracker & Ability to track multiple objects & Survived they occlusion\\
\hline
SIMPLEBACKGROUND & No & No \\
MIL & Yes & No \\
BOOSTING & Yes &  No\\
TLD & Yes* & Yes \\
MEDIANFLOW & Yes & No \\
HSV & Only different colors & Yes \\
PATTERNMATCHING & Only different patterns* & Yes \\
CORRELATION & Yes & No \\
MOSSE & Yes & No \\
\end{tabular}
\label{table:multiple-objects}
\end{table}
* Trackers performed badly

\begin{figure}
\centering
\begin{subfigure}{0.48\linewidth}
\includegraphics[width=\linewidth]{img/experiments/color-occlusion.png}
\caption{}
\end{subfigure}
\begin{subfigure}{0.48\linewidth}
\includegraphics[width=\linewidth]{img/experiments/occlusion.png}
\caption{}
\end{subfigure}
\label{fig:occlusion}
\caption{Example of successfully recovering from the full occlusion}
\end{figure}

\subsection{Conlusion}

Previous subsection gave us comparison of the trackers. Still, there is not one
best tracker, but it might be to consider using specific tracker in occasion.

Considering the results, we can easily exclude the trackers MIL, BOOSTING, TLD
because of their low speed. 

If the object has some area one colored, the HSV tracker is a great choice. The
position of the result is little bit shifted from the center of the object, but
the tracker is stable, fast and even report object lost.

Unfortunately, most of the objects share the color of the background, or do not
have one colored are on themself. Then we have to choose from the other
trackers.

If the objects move autonomously, the light is stable and we track only one
object, Simple Background tracker server it purposes well. Pattern Matching
tracker works well with non-noisy background.

In conclusion, it is best to try several of them and find the best suiting one.
Their different approaches makes them good in some situations but unusable in
other. It also depends on the situation, it might be more important to have
precise tracker, even at cost of loosing the object, if the operator of the
program can correct it during the run. In other situation may many occlusion of
the tracked objects occur, and TLD tracker even it is slow, might save the
situation.

Some tips at the end:
\begin{itemize}
\item if the object is autonomous -- use Simple Background
\item if the object has onecolor area -- use HSV
\item if the mutual occlusion of the objects may appear -- try TLD
\item otherwise, it is worth to try MEDIANFLOW, CORRELATION and sometimes also MOSSE, PATTERNMATCHING
\end{itemize}
