\chapter{Tracker}

We consider a tracker to be an algorithm for detection a position of an object in
an image. We present a few tested trackers and their results in this task.
Firstly, we provide a short description of a simple straightforward tracker and
then we describe more complicated trackers.

Tracker returns an object position in the image. We differentiate between two
types of tracking algorithms: \emph{detection based algorithms} and
\emph{sequence-based algorithms}. Detection based algorithms detect an object
on each image separately. On the other hand, sequence-based algorithms obtain,
store and process information from a sequence of the past images and use them
for more accurate results, while requiring same or even less computation time
compared to the detection based algorithms. For this task, we test few trackers
in both categories. Evaluation of the trackers is at the end of this chapter.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section {Detection based algorithms}

We denote all trackers, which detect an object on each image separately, as
detection based algorithms. These algorithms use only information obtained from
the current image and the information from the image used while the tracker was
initialized.

\subsection{Simple Background Tracker}

\emph{Simple Background Tracker} takes a photo of the background at the
beginning and we denote it as a \emph{pattern}. In order to detect an object in
an \emph{image}, a comparison of the \emph{image} and the \emph{pattern} is done.
We make this comparison by taking a sum of an absolute difference for each
color channel (Red, Green, Blue) in the images for each pixel.

As a result, we get a mask, where higher values mean the bigger difference between
the colors of the \emph{pattern} and the \emph{image} at given pixel. We assume it
is caused by an object in front of the camera.

One may expect to get the mask by \emph{tresholding} (see the results in
\ref{fig:simple-background-first-threshold}). There is still a lot of noise.
The noise is in the form of small dots and lines. Therefore we use
\emph{blurring} to remove the noise. Then we \emph{threshold} it again to get
binary mask (see the results in \ref{fig:simple-background-second-threshold}).
At this point, we will find a contour with the biggest area using OpenCV
library. The center point of the rectangle of this contour will be our
estimation of the position of our object in the image. The whole process is in
the figure \ref{fig:simple-background-tracker}.

In the image, it is showed that between the photo of the background and the photo
of the object the light slightly changed (at the edge of the puzzle pieces). It
is causing noise, which we reduce by thresholding and blurring the image.

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=\linewidth]{img/simple_background/background.jpg}
    \caption{Background}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=\linewidth]{img/simple_background/object.jpg}
    \caption{Object}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=\linewidth]{img/simple_background/rgb-diff.jpg}
    \caption{Sum of diffs in each channel (RGB)}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=\linewidth]{img/simple_background/first-thresh.jpg}
    \caption{Thresholded}
    \label{fig:simple-background-first-threshold}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=\linewidth]{img/simple_background/blurring.jpg}
    \caption{Blurred}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=\linewidth]{img/simple_background/second-thresh.jpg}
    \caption{Thresholded}
    \label{fig:simple-background-second-threshold}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=\linewidth]{img/simple_background/result.jpg}
    \caption{Bounding box of the largest contour found.}
  \end{subfigure}
  \caption{Process of the simple background tracker}
  \label{fig:simple-background-tracker}
\end{figure}

As an advantage of this algorithm, we consider its simplicity and
straight-forward implementation. Furthermore, with a static background with
only one object moving it can reliably track an object without any information
about it.

On the contrary, it can not recover even from a little light changes or camera
movement. Also, an object moved by a human, for example, cannot be tracked
reliably, since the tracker recognizes the hand as a moving object
too.

\subsection{HSV tracker}

HSV tracker uses an idea of tracking an object by its color. Given an input
object described by a bounding box, we find the average color within the
bounding box. On a position request, we return a center of the largest area
with the color of the object.

We choose the color coding via HSV (Hue, Saturation, Value). Unlike the RGB (Red,
Green, Blue) coding it can describe a color as hue value, not triple values of
mixed colors. The approach of HSV color coding preserves one value -- hue
value,  even though the color is lighter or darker (like shadows in the image).
On the other hand in the RGB coding shadows may cause a difference in all three
parts of coded color. Therefore an object description can be simplified
to one value.

\begin{figure}[h!]\centering
\includegraphics[width=0.65\textwidth]{img/hsv-cylinder.png}
\caption{"HSV cylinder" by SharkD is licensed under CC BY 3.0}
\end{figure}

We describe the algorithm in a few steps. Firstly, we convert the template
image (bounding box) from the RGB color space to HSV. Then we choose an average
color in the template. Since the coding of hue part is placed in the circle, it
is not enough to take a commonly used average. It would cause that image full
of warm red (the hue value of this color is circa equal to 15) and cool red
(the hue is circa 345) would average to mid cyan (hue: 180), instead of Red
(hue: 0).

To get a more reasonable average, we take the hue value of each pixel as an
unit vector (the hue value is encoded as the angle between given vector
and vector $(1, 0)$). We sum these vectors and get a vector $(x, y)$. We find a
corresponding angle for this vector using $arctang2$ function. The following
formulas describe the process of getting an average angle.

\todo[inline]{Obrazok na ten vzorceky a tak}

$$
\begin{aligned}
x &= \sum_\alpha cos \alpha \\
y &= \sum_\alpha sin \alpha \\
\alpha_{avg} &= arctang2(y, x)
\end{aligned}
$$

The use of this algorithm for one-colore object is displayed in the figure
\ref{fig:hsv-tracker}. We consider as a disadvantage that no other objects of
the same color can be placed in the view of the camera.

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=\linewidth]{img/hsv/initial.jpg}
    \caption{Initial image with the selected object}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=\linewidth]{img/hsv/object.jpg}
    \caption{Image with the moved object}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=\linewidth]{img/hsv/mask.jpg}
    \caption{Mask created by looking for similar colors}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=\linewidth]{img/hsv/result.jpg}
    \caption{Bounding box of the largest contour found.}
  \end{subfigure}
  \caption{Process of the HSV tracker}
  \label{fig:hsv-tracker}
\end{figure}

\subsection {Pattern matching}

Pattern (or template) matching algorithm slides over the input image and
compares the template with the patch of the input image. By \emph{patch} we
mean an image cropped to the size of the template (displayed in the figure
\ref{fig:patternmatching-naming}).

For a comparison between an image and the patch we look for a function, which
tell us how different to each other they are. Such a function is usually called
a loss fuction. We use a function $R$ based on square distance as our loss
function. More precisely, we take the sum of square distances of all pairs of
corresponding pixels in the template and the patch. Therefore, we compute $R$
as:

$$
R(x, y) =
%\frac{
\sum_{x', y'} ||T(x', y') - I(x + x', y + y')||^2
%}{
%\sqrt{\sum_{x', y'} T(x', y')^2 \cdot \sum_{x', y'} I(x+x', y+y')^2}
%}
$$
where $x'$ and $y'$ denote points from the neighbourhood of the $x, y$. $T$
denotes our pattern and $I$ denotes the image. 

\begin{figure}[h]
	\centering
	\def\svgwidth{0.9\linewidth}
	\input{img/pattern_matching/naming.pdf_tex}
	\caption{Naming convention showed on an example}
	\label{fig:patternmatching-naming}
\end{figure}

From the computed value for each pixel the one with the lowest value
(i.e. shortest distance) is our estimation of the position of the object.

Finally, we choose an appropriate representation for the pixels (i.e. what
precisely $T(x, y)$ and $I(x, y)$ stand for). In general, we can choose any
reasonable vector (such as a tuple of RGB channels). We choose to use a standard
conversion to grayscale ($T(x, y)$ and $I(x, y)$ thus give an intensity after
such conversion).

Because this algorithm works with the grayscale images, much information is
lost during conversion. This disadvantage summed up with no ability to
recognize rotated or slightly changed objects results in unsatisfying tracking
results. A process of correct match and also incorrect one is displayed in the
figure \ref{fig:pattern-matching-tracker}. We can see the masks, which displays
the value of the loss function. Lower values of the loss functions are
displayed as darker points. Therefore, the black spots means patches which are
similar to the pattern.

\begin{figure}
  \centering
  \begin{subfigure}{0.48\linewidth}
    \includegraphics[width=\linewidth]{img/pattern_matching/initial.jpg}
    \caption{Initial image with selected object}
  \end{subfigure}
  \begin{subfigure}{0.48\linewidth}
    \includegraphics[width=\linewidth]{img/pattern_matching/blackwhite.jpg}
    \caption{Converting it to black white}
  \end{subfigure}
  \begin{subfigure}{0.48\linewidth}
    \includegraphics[width=\linewidth]{img/pattern_matching/mask.jpg}
    \caption{Mask created by applying metric}
  \end{subfigure}
  \begin{subfigure}{0.48\linewidth}
    \includegraphics[width=\linewidth]{img/pattern_matching/result-correct.jpg}
    \caption{Darkest point (lowest value) from mask is choosed}
  \end{subfigure}
  \begin{subfigure}{0.48\linewidth}
    \includegraphics[width=\linewidth]{img/pattern_matching/mask-incorrect.jpg}
    \caption{Mask with darkest point on the left}
  \end{subfigure}
  \begin{subfigure}{0.48\linewidth}
    \includegraphics[width=\linewidth]{img/pattern_matching/result-incorrect.jpg}
    \caption{Incorrectly matched pattern}
  \end{subfigure}
  \caption{Process of the pattern matching}
  \label{fig:pattern-matching-tracker}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Sequence-based algorithms}

We denote as sequence-based algorithms a class of tracking algorithms, which
are using information from a sequence of images.

Using the advantage of information from previous frames one could create not only
more stable but also faster trackers. Furthermore, trackers of this class usually preserve identity, which means
that also in the case of multiple moving objects in a frame it remains tracking the original
one.

Examples of the information we can obtain from sequence of the images are:
\begin{itemize}
\item velocity -- from previous images we can estimate the speed and direction of
  the movement. This information can reduce the searching area to smaller one
  and increase the speed of the algorithm.
\item appearance -- the object may rotate and change its shape or color. Tracker able to
  learn can be persistable against such changes
\end{itemize}

An algorithm using this information can cope with occlusion (one object covers
another) -- what detection algorithms are usually not able to do.

In the next sections, we will present a few trackers implemented in the OpenCV. We
provide a short overview of the trackers available.

\subsection*{BOOSTING tracker}
\todo[inline]{Citace?}
Boosting tracker is based on online AdaBoost. It considers a bounding box as
a positive sample and patches of background as negative ones. For a new image, the
classifier runs on every pixel in the neighborhood of the previous location,
scoring every pixel. The location with the highest score is chosen as a new
location. The implementation in OpenCV is based on \citet*{boosting}.

\subsection*{MIL tracker}
The MIL is an abbreviation for Multiple Instance Learning. In comparison to the
BOOSTING tracker, it does not keep only one image of the positive example, but the
set of images. The tracker considers a small neighborhood of the
current position as possible positive examples. It helps the tracker to cope with
the occlusion. OpenCV implementation is based on \citet*{mil}.

\subsection*{KCF tracker}
KCF stands for Kernelized Correlation Filters. Similarly, as the MIL tracker,
it uses more positive samples and their large overlapping regions.
Implementation provided by OpenCV is based on \citet*{kcf}.

\subsection*{TLD tracker}
Tracking, learning and detection, these are the three components of this
tracker. The tracker works frame to frame, and detection is run to correct the
tracker if necessary. The learning estimates detector's errors and updates it
to avoid these errors in the future. The implementation in OpenCV is based on
\citet*{tld}.

\subsection*{MEDIANFLOW tracker}
This tracker focuses on forward-backward error trying to minimize it. The
implementation in OpenCV is based on \citet*{medianflow}.

\subsection*{MOSSE tracker}
MOSSE tracker is proposed for fast object tracking using correlation filter
methods. Firstly it does Fast Fourier Transform for template and the image.
Then an convolution operation is performed between the images and the result is
inverted by Inverse Fast Fourier Transform (IFFT). The position is estimated by
the highest value of the IFFT response. More about the tracker is available in
the paper by \citet*{mosse}.

\subsection*{OpenCV note}
OpenCV-contribute implements all above-described sequence-based trackers. An
overview of OpenCV trackers is provided by \citet*{opencv-trackers}.

\subsection*{Correlation tracker}
We decided to include a tracker implemented in Dlib. This
correlation tracker is based on the paper by \citet*{correlation}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Trackers evaluation}

In the previous section, we described many different trackers. Now we provide
their evaluation of several experiments. Our goal is to find the
best-performing ones. The trackers differ in the approach to obtain information
from the video stream, so it is important to test them under real conditions.

Firstly, we define the key abilities which are essential for our project. Then
we provide a description and the results from several experiments. At the end
of this section, in conclusion, we provide few pieces of advice how to choose
the best tracker for a specific environment.

\subsection*{Key abilities of the tracker}

The trackers differ, therefore they can outperform others in some specific
situations. To be able to compare two trackers, we provide a list of the most
important to us in our project.

\begin{itemize}
	\item Accuracy
	\item Speed
	\item Ability to recover from the occlusion
	\item Ability to track multiple objects
\end{itemize}

Following paragraphs describe their importance and also the way we measure them
in our experiments.

\subsubsection*{Accuracy}

The good tracker has to be accurate. Tracker which is not accurate does not
satisfy its purpose. Furthermore, the accuracy is important to obtain good
localization results, since the inaccuracy of the estimated position of the
object in the image cause inaccuracy in the estimated position in the 3D space.

It is quite difficult to say what is accuracy in the tracker case. Hence, we
measure \emph{inaccuracy} instead. Inaccuracy in our case is represented by the
distance (in pixels) from the true position of the object and the one provided
by the tracker. Higher values mean less accurate tracker.

To obtain robust results (not depending on one image), we take many images and
compute the inaccuracy for each of them. Then the mean of this inaccuracy is
the estimated inaccuracy for the tracker. The value is dependend on the
resolution of the images. In our case we use images 640 $\times$ 480 px.

The last problem remaining is to get the right position of the object. One
approach is to get the position by selecting the object by a human. This
approach is very time consuming because only a one-minute long video contains
approximately 1800 frames.

Instead of this approach, we select the best tracker. We do it by looking at
the tracking results on the video, and we choose the one which tracked the
whole video correctly. This representative tracker has imprecision equal to 0.
The inaccuracy of the other trackers is computed as a distance between the
selected object by this tracker and the representative tracker. 

\subsubsection*{Speed/Computational time}

Trackers differ not only in their accuracy but naturally also in the time
needed to process an image to find the object. To measure the tracker's speed,
we measure this time. We measure it as a number of ticks passed during calling
tracker update (get the position on the given image). Then we take the number
of the ticks per second and divide it by the number of ticks needed for tracker
update. This way we obtain the number of the images that could be processed in
one second by this tracker. Again, we do it on many images and then take a mean
of these values.

For the number of images that could be processed in one second, we use a
shortcut FPS, coming from Frames Per Second. A higher number means a higher
speed of the tracker, therefore less time needed for each image.

This value is highly dependable on the computer and the operating system.
Therefore, we use the value of FPS as a rough guess.

\todo[inline]{Mozno napisat na akom stroji bolo toto ratane}

Choosing a tracker by its speed is important. We consider a tracker too slow if
it is not able to track alive (>30~FPS). Even if the tracker has 30~FPS in the
experiment, it might be too slow in the application, since it has to perform
many other tasks. Slow tracker cause lag the whole application since much
computational power is used for the tracking.

\subsubsection*{Recovering from the occlusion}

We refer to occlusion as a partially or fully covering tracked object by
another one, or leaving the view of the camera.

During the tracking, it might come handy to have a tracker, which can recover
from the occlusion. It is not easy, to keep the object visible in both cameras.
Therefore, we are interested also in the ability to recover from the occlusion.

To test this ability, we watched the videos with tracking results and decided
if it can recover or not.

As another useful ability is to detect if the object was lost. It is better to
report object loss than providing incorrect results. We tested it together with
recovering.

\subsubsection*{Tracking multiple objects}

As an addition to our project, our program can localize multiple objects. When
working with multiple objects, it is important to remember that not all
trackers may be able to track correctly multiple objects at the same time.

More problem arises if the tracking objects occlude each other since not all
trackers can handle it.

We observed this ability by the human eye.

With tracking multiple objects, the speed of the program may decrease rapidly.
It is important to remember, that each object in each camera view has own
tracker. Therefore the tracking part takes more time.

In the next section, we provide an overview of the experiment used to estimate
tracker abilities.

%%%%%%%%%%%%

\subsection{Experiments and the results}

We know describe the experiments use to measure tracker's statistics. Since the
main use of our project can be to track a robots and record their trajectory,
we decided to test the trackers under similar circumstances.

In some experiments we use autonomous robot, which has the shape of oval, 3~cm
height and having 6~cm in diameter. This robot is able to follow a black line.
We used a oval as the shape of the line (see the figure \ref{fig:robot-oval}).

In order to test the trackers under same conditions, we recorded a video of the
robot. We then found the bounding box for the object at the start and
initialized the trackers on the same video with same bounding box at the
beginning. We made only some exception because of the requirements of the some
trackers, which are mentioned in the corresponding experiment.

\subsubsection{Speed and accuracy}

The first experiment is to estimate tracker's speed and accuracy. We used a
video of the moving robot in shape of oval. For the accuracy measurement we
choosed a Simple Background tracker as our representative, since it provided an
excellent example of the correct tracking on this video. The representative
tracker is always displayed as a red bounding box and the blue is used for
tested tracker.

Since the Simple Background trackers need an empty background, when
initializing, we passed an image without a robot to it.

\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{img/robot-oval.png}
\caption{Selecting object to track on the video}
\label{fig:robot-oval}
\end{figure}

We consider this experiment as quite difficult for the trackers, since the
robot is not only moving, but also quite fast changing its appearance. In a
second or two, its top platform is mirrored. On the other hand, the object does
not change size and the background is very clear.

The results of the experiment are available in the table \ref{table:experiment-robot}.

\begin{table}
\centering
\begin{tabular}{l|l|l}
Tracker	& FPS & Inaccuracy (px) \\
\hline
\input{experiments/trackers.txt}
\end{tabular}
\caption{Estimation of the trackers speed and inaccuracy.}
\label{table:experiment-robot}
\end{table}

\begin{figure}
\centering
\begin{subfigure}{0.48\linewidth}
\includegraphics[width=\linewidth]{img/experiments/hsv-select.png}
\caption{Selecting bounding box for HSV tracker}
\label{fig:hsv-bbox}
\end{subfigure}
\begin{subfigure}{0.48\linewidth}
\includegraphics[width=\linewidth]{img/experiments/hsv-tracking.png}
\caption{Simple background tracker (red) and HSV tracker (blue)}
\label{fig:hsv-diff}
\end{subfigure}
\caption{Testing HSV tracker}
\end{figure}

\begin{table}
\centering
\begin{tabular}{l|l|l}
Tracker	& FPS & Inaccuracy (px) \\
\hline
\input{experiments/hsv.txt}
\end{tabular}
\caption{TODO caption}
\label{table:hsv}
\end{table}

From the results, we can see that MIL, BOOSTING and TLD trackers are too slow
for our application which runs live. HSV tracker is not able to perform at
all, since the robot is multicolored object.

We can also see that, even the MIL and BOOSTING trackers were slow, they
performed quite well. The trackers which have mean value of accuracy more than
100 lost the object at some point and were not aware of it.

As a result from this experiment, we consider CORRELATION, SIMPLEBACKGROUND,
MEDIANFLOW, PATTERMATCHING as usable for our purposes.

To test HSV tracker, we modified the experiment by with placing orange paper to
track on the top of the robot (see figure \ref{fig:hsv-bbox}). The obtained
values for this tracker are listed in the table \ref{table:hsv}. For improving
the performance of the HSV tracker, it is better to select smaller area with
the same color. Therefore, the tracker will track only this are, not the whole
object. The difference between the are of the SIMPLEBACKGROUND and the HSV
tracker is displayed in the figure \ref{fig:hsv-diff}. The red line symbolises
Simple Background tracker and the blue one the HSV tracker. The resulted
accuracy is caused by tracking specific part of the object, since the tracker
performed very well.

\todo[inline]{Netusim, ci pisat nazvy trackerov velkym alebo malym, niektore su fakt srkatky, u inych to neviem...}

\subsection{Object under occlusion}

As we have mentioned earlier, it might come handy, to have a tracker which is
able to recover from occlusion. We prepared an experiment, which loose the
robot from the view. We were interested if the tracker is able to report object
lost (or returns wrong results) and if it was able to track the object again
after coming back to camera view. The example of the successful recover from
the occlusion is displayed in the figure \ref{fig:occlusion}.

\begin{figure}
\centering
\begin{subfigure}{0.48\linewidth}
\includegraphics[width=\linewidth]{img/experiments/occlusion1.png}
\end{subfigure}
\begin{subfigure}{0.48\linewidth}
\includegraphics[width=\linewidth]{img/experiments/occlusion2.png}
\end{subfigure}
\caption{Example of successfully recovering from the full occlusion}
\label{fig:occlusion}
\end{figure}

\subsection{Tracking multiple objects}

The last test for the trackers is ability to track multiple objects. Simple
detection-based trackers usually choose the best position with highest area
satisfing given condition, therefore tracking multiple object is not possible.

In such situation when tracking multiple object is needed, sequence-based
trackers may outperform detection-based trackers. We decided to test their
ability to keep tracking the same object, the results are listed in the table
\ref{table:occlusion-multiple} in the fourth and fifth columns.

Most of the trackers are not able to recover from mutual occlusion. Denote as
the first object the object behind and the second the one in front of the
first. Then the second object approach first, the tracker of the first object
learn fase of the changing shape of the object and therefore the tracker start
to track the second object. This situation could not resolve most of the
trackers correctly.

\begin{figure}
\centering
\begin{subfigure}{0.48\linewidth}
\includegraphics[width=\linewidth]{img/experiments/color-occlusion.png}
\caption{Successful (HSV)}
\end{subfigure}
\begin{subfigure}{0.48\linewidth}
\includegraphics[width=\linewidth]{img/experiments/occlusion.png}
\caption{Unsuccessful (MIL)}
\end{subfigure}
\caption{Example of recoverings from the full occlusion}
\label{fig:occlusion}
\end{figure}

\subsection{Conlusion}

Previous subsection gave us comparison of the trackers. Still, there is not one
best tracker, but it might be to consider using specific tracker in occasion.

Considering the results, we can easily exclude the trackers MIL, BOOSTING, TLD
because of their low speed. 

If the object has some area one colored, the HSV tracker is a great choice. The
position of the result is little bit shifted from the center of the object, but
the tracker is stable, fast and even report object lost.

Unfortunately, most of the objects share the color of the background, or do not
have one colored are on themself. Then we have to choose from the other
trackers.

If the objects move autonomously, the light is stable and we track only one
object, Simple Background tracker server it purposes well. Pattern Matching
tracker works well with non-noisy background.

In conclusion, it is best to try several of them and find the best suiting one.
Their different approaches makes them good in some situations but unusable in
other. It also depends on the situation, it might be more important to have
precise tracker, even at cost of loosing the object, if the operator of the
program can correct it during the run. In other situation may many occlusion of
the tracked objects occur, and TLD tracker even it is slow, might save the
situation.

Some tips at the end:
\begin{itemize}
\item if the object is autonomous -- use Simple Background
\item if the object has onecolor area -- use HSV
\item if the mutual occlusion of the objects may appear -- try TLD
\item otherwise, it is worth to try MEDIANFLOW, CORRELATION and sometimes also MOSSE, PATTERNMATCHING
\end{itemize}

\begin{landscape}
\begin{table}
\centering
\begin{tabular}{p{5cm}|p{3cm}|p{4cm}|p{5cm}|p{5cm}}
Tracker & Report object lost & Recovers from full occlusion & Ability to track multiple objects & Able to track correctly after mutual occlusion of tracked objects \\
\hline
SIMPLEBACKGROUND & Yes & Yes & No & No \\
MIL & No & No  & Yes & No \\
BOOSTING & No & No & Yes & No \\
TLD & Sometimes & Yes & Yes* & No \\
MEDIANFLOW & Yes & No & Yes & No \\
HSV & Yes & Yes & Only different colors & Yes \\
PATTERNMATCHING & No & Yes & Only different patterns* & Yes \\
CORRELATION & No & No & Yes & No \\
MOSSE & Sometimes & No & Yes & No
\end{tabular}
\vspace{1ex}
\raggedright *Tracker performed badly
\caption{Summary of the trackers abilities}
\label{table:occlusion-multiple}
\end{table}
\end{landscape}
